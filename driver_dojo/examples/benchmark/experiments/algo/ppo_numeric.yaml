name: "ppo"
params:
    visual: False
    buffer_size: 100000
    lr: 0.0003
    lr_decay: False
    gamma: 0.99
    gae_lambda: 0.95
    batch_size: 256
    hidden_sizes:
        - 256
        - 256
    eps_clip: 0.2
    dual_clip: null
    value_clip: False
    advantage_normalization: True
    recompute_advantage: False
    vf_coef: 0.5
    ent_coef: 0.01
    max_grad_norm: null
    reward_normalization: False
    max_batchsize: 256
    action_scaling: True
    max_action: 1
    step_per_collect: 2048  # 300 steps per parallel env per update for 16 envs
    repeat_per_collect: 5
    step_per_epoch: 100000
    epoch: 100
    training_num: 8
    test_num: null
    device: "cpu"
    seed: 1337
